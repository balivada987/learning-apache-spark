---
title: "Data Wrangling"
author: "Wenqiang Feng & Ming Chen"
date: "2/19/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval=FALSE)
```

## Set up sparkContext and sparkSession

```{python}
# connecting to spark
from pyspark import SparkConf, SparkContext
## set up spark context
conf = SparkConf().setAppName("myApp")
sc = SparkContext(conf=conf)

# create sparksession object
from pyspark.sql import SparkSession
sparksession = SparkSession(sc)
```

## Import data

```{python}
# import data
# read data from csv
# iris = sparksession.read.csv("data/iris.csv", inferSchema=True, header=True)
mtcars = sparksession.read.csv("data/mtcars.csv", inferSchema=True, header=True)
mtcars.show(n=5)
```

```{python}
+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
|              _c0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|
+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
|        Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|
|    Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|
|       Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|
|   Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|
|Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|
+-----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
```


## Data dimensions

* **Column number**

```{python}
len(mtcars.columns)
```

```{python}
12
```

* **Row number**
```{python}
mtcars.count()
```

```{python}
32
```


## Select columns

* **Select columns by column names**

```{python}
mtcars.select(['mpg', 'cyl']).show(n=5)
```

```{python}
+----+---+
| mpg|cyl|
+----+---+
|21.0|  6|
|21.0|  6|
|22.8|  4|
|21.4|  6|
|18.7|  8|
+----+---+
only showing top 5 rows
```

* **Select columns by indices**

```{python}
mtcars.rdd.map(lambda x: [x[i] for i in range(1,6)]).toDF().show(n=5)
```

```{python}
+----+---+-----+---+----+
|  _1| _2|   _3| _4|  _5|
+----+---+-----+---+----+
|21.0|  6|160.0|110| 3.9|
|21.0|  6|160.0|110| 3.9|
|22.8|  4|108.0| 93|3.85|
|21.4|  6|258.0|110|3.08|
|18.7|  8|360.0|175|3.15|
+----+---+-----+---+----+
only showing top 5 rows
```

```{python}
mtcars.rdd.map(lambda x: [x[i] for i in (1,3,6,7)]).toDF().show(n=5)
```

```{python}
+----+-----+-----+-----+
|  _1|   _2|   _3|   _4|
+----+-----+-----+-----+
|21.0|160.0| 2.62|16.46|
|21.0|160.0|2.875|17.02|
|22.8|108.0| 2.32|18.61|
|21.4|258.0|3.215|19.44|
|18.7|360.0| 3.44|17.02|
+----+-----+-----+-----+
only showing top 5 rows
```

## Rename columns

* **Rename single column**

```{python}
mtcars.withColumnRenamed('mpg', 'new_mpg')
```

```{python}
+-----------------+-------+---+-----+---+----+-----+-----+---+---+----+----+
|              _c0|new_mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|
+-----------------+-------+---+-----+---+----+-----+-----+---+---+----+----+
|        Mazda RX4|   21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|
|    Mazda RX4 Wag|   21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|
|       Datsun 710|   22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|
|   Hornet 4 Drive|   21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|
|Hornet Sportabout|   18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|
+-----------------+-------+---+-----+---+----+-----+-----+---+---+----+----+
```

* **Rename multiple columns**

```{python}
new_mtcars = mtcars.rdd.map(lambda x: [x[i] for i in range(1,6)]).toDF()
new_mtcars.show(5)
```

```{python}
+----+---+-----+---+----+
|  _1| _2|   _3| _4|  _5|
+----+---+-----+---+----+
|21.0|  6|160.0|110| 3.9|
|21.0|  6|160.0|110| 3.9|
|22.8|  4|108.0| 93|3.85|
|21.4|  6|258.0|110|3.08|
|18.7|  8|360.0|175|3.15|
+----+---+-----+---+----+
only showing top 5 rows
```

```{python}
## reset column names
new_column_names = [mtcars.columns[i] for i in range(1, 6)]
ori_column_names = new_mtcars.columns
column_names = zip(ori_column_names, new_column_names)
for i in range(0, len(new_mtcars.columns)):
    new_mtcars = new_mtcars.withColumnRenamed(column_names[i][0], column_names[i][1])
```

```{python}
new_mtcars.show(n=5)
```

```{python}
+----+---+-----+---+----+
| mpg|cyl| disp| hp|drat|
+----+---+-----+---+----+
|21.0|  6|160.0|110| 3.9|
|21.0|  6|160.0|110| 3.9|
|22.8|  4|108.0| 93|3.85|
|21.4|  6|258.0|110|3.08|
|18.7|  8|360.0|175|3.15|
+----+---+-----+---+----+
only showing top 5 rows
```

## Get unique elements in a column

```{python}
mtcars.select(['carb']).distinct().show()
```

```{python}
+----+
|carb|
+----+
|   1|
|   6|
|   3|
|   4|
|   8|
|   2|
+----+
```


## Split data into training and test sets

```{python}
training, test = mtcars.randomSplit(weights=[0.75, 0.25], seed=24)
```

* **Training set**
```{python}
training
```

```{python}
+----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
|             _c0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|
+----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
|      Camaro Z28|13.3|  8|350.0|245|3.73| 3.84|15.41|  0|  0|   3|   4|
|      Datsun 710|22.8|  4|108.0| 93|3.85| 2.32|18.61|  1|  1|   4|   1|
|    Ferrari Dino|19.7|  6|145.0|175|3.62| 2.77| 15.5|  0|  1|   5|   6|
|        Fiat 128|32.4|  4| 78.7| 66|4.08|  2.2|19.47|  1|  1|   4|   1|
|       Fiat X1-9|27.3|  4| 79.0| 66|4.08|1.935| 18.9|  1|  1|   4|   1|
|  Ford Pantera L|15.8|  8|351.0|264|4.22| 3.17| 14.5|  0|  1|   5|   4|
|  Hornet 4 Drive|21.4|  6|258.0|110|3.08|3.215|19.44|  1|  0|   3|   1|
|    Lotus Europa|30.4|  4| 95.1|113|3.77|1.513| 16.9|  1|  1|   5|   2|
|       Mazda RX4|21.0|  6|160.0|110| 3.9| 2.62|16.46|  0|  1|   4|   4|
|   Mazda RX4 Wag|21.0|  6|160.0|110| 3.9|2.875|17.02|  0|  1|   4|   4|
|       Merc 240D|24.4|  4|146.7| 62|3.69| 3.19| 20.0|  1|  0|   4|   2|
|        Merc 280|19.2|  6|167.6|123|3.92| 3.44| 18.3|  1|  0|   4|   4|
|       Merc 280C|17.8|  6|167.6|123|3.92| 3.44| 18.9|  1|  0|   4|   4|
|      Merc 450SL|17.3|  8|275.8|180|3.07| 3.73| 17.6|  0|  0|   3|   3|
|     Merc 450SLC|15.2|  8|275.8|180|3.07| 3.78| 18.0|  0|  0|   3|   3|
|Pontiac Firebird|19.2|  8|400.0|175|3.08|3.845|17.05|  0|  0|   3|   2|
|  Toyota Corolla|33.9|  4| 71.1| 65|4.22|1.835| 19.9|  1|  1|   4|   1|
|   Toyota Corona|21.5|  4|120.1| 97| 3.7|2.465|20.01|  1|  0|   3|   1|
|      Volvo 142E|21.4|  4|121.0|109|4.11| 2.78| 18.6|  1|  1|   4|   2|
+----------------+----+---+-----+---+----+-----+-----+---+---+----+----+
```

* **Test set**

```{python}
test
```

```{python}
+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+
|                _c0| mpg|cyl| disp| hp|drat|   wt| qsec| vs| am|gear|carb|
+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+
|        AMC Javelin|15.2|  8|304.0|150|3.15|3.435| 17.3|  0|  0|   3|   2|
| Cadillac Fleetwood|10.4|  8|472.0|205|2.93| 5.25|17.98|  0|  0|   3|   4|
|  Chrysler Imperial|14.7|  8|440.0|230|3.23|5.345|17.42|  0|  0|   3|   4|
|   Dodge Challenger|15.5|  8|318.0|150|2.76| 3.52|16.87|  0|  0|   3|   2|
|         Duster 360|14.3|  8|360.0|245|3.21| 3.57|15.84|  0|  0|   3|   4|
|        Honda Civic|30.4|  4| 75.7| 52|4.93|1.615|18.52|  1|  1|   4|   2|
|  Hornet Sportabout|18.7|  8|360.0|175|3.15| 3.44|17.02|  0|  0|   3|   2|
|Lincoln Continental|10.4|  8|460.0|215| 3.0|5.424|17.82|  0|  0|   3|   4|
|      Maserati Bora|15.0|  8|301.0|335|3.54| 3.57| 14.6|  0|  1|   5|   8|
|           Merc 230|22.8|  4|140.8| 95|3.92| 3.15| 22.9|  1|  0|   4|   2|
|         Merc 450SE|16.4|  8|275.8|180|3.07| 4.07| 17.4|  0|  0|   3|   3|
|      Porsche 914-2|26.0|  4|120.3| 91|4.43| 2.14| 16.7|  0|  1|   5|   2|
|            Valiant|18.1|  6|225.0|105|2.76| 3.46|20.22|  1|  0|   3|   1|
+-------------------+----+---+-----+---+----+-----+-----+---+---+----+----+
```

* **Dataset can be randomly split into any number of subsets**
```{python}
df1, df2, df3, df4 = mtcars.randomSplit(weights=[0.2, 0.2, 0.15, 0.25], seed=123)
df1.count()
df2.count()
df3.count()
df4.count()
```

###  Data Stack 

- spread dense data into columns
```{python eval=FALSE}
df = spark.sparkContext.parallelize([
    ("assert", Vectors.dense([1, 2, 3]))
]).toDF(["word", "vector"])
```
```{python eval=FALSE}
df.show()
```

```{python eval=FALSE}
+------+-------------+
|  word|       vector|
+------+-------------+
|assert|[1.0,2.0,3.0]|
+------+-------------+
```

- main function
```{python eval=FALSE}
from pyspark.ml.linalg import Vectors
def extract(row):
    return (row.word, ) + tuple(float(x) for x in row.vector.values)
```


```{python eval=FALSE}
df_new = df.rdd.map(extract).toDF(["word"]) 
```
```{python eval=FALSE}
df_new.show()
```

```{python eval=FALSE}
#ouput 
+------+---+---+---+
|  word| _2| _3| _4|
+------+---+---+---+
|assert|1.0|2.0|3.0|
+------+---+---+---+
```


- spread dense data into rows

```{python eval=FALSE}
df = spark.sparkContext.parallelize([(1, 2, 3, 'a b c'),
                     (4, 5, 6, 'd e f'),
                     (7, 8, 9, 'g h i')]).toDF(['col1', 'col2', 'col3','col4'])
```
```{python eval=FALSE}
df.show()
```

```{python eval=FALSE}
# output
+----+----+----+-----+
|col1|col2|col3| col4|
+----+----+----+-----+
|   1|   2|   3|a b c|
|   4|   5|   6|d e f|
|   7|   8|   9|g h i|
+----+----+----+-----+
```
```{python eval=FALSE}
df.printSchema()
```
```{python eval=FALSE}
# output
root
 |-- col1: long (nullable = true)
 |-- col2: long (nullable = true)
 |-- col3: long (nullable = true)
 |-- col4: string (nullable = true)
```

- main function 
```{python eval=FALSE}
from pyspark.sql.functions import split, explode
new = df.withColumn('col4',explode(split('col4',' ')))
```

```{python eval=FALSE}
new.show()
```

```{python eval=FALSE}
+----+----+----+----+
|col1|col2|col3|col4|
+----+----+----+----+
|   1|   2|   3|   a|
|   1|   2|   3|   b|
|   1|   2|   3|   c|
|   4|   5|   6|   d|
|   4|   5|   6|   e|
|   4|   5|   6|   f|
|   7|   8|   9|   g|
|   7|   8|   9|   h|
|   7|   8|   9|   i|
+----+----+----+----+
```

### Dataframes joint

- demo dataframes 
```{python eval=FALSE}
a = spark.sparkContext.\
    parallelize([['a', 'foo'], ['b', 'hem'], ['c', 'haw']]).toDF(['a_id', 'extra'])
```
```{python eval=FALSE}
a.show()
```


```{python eval=FALSE}
# output 
+----+-----+
|a_id|extra|
+----+-----+
|   a|  foo|
|   b|  hem|
|   c|  haw|
+----+-----+
```


```{python eval=FALSE}
b = spark.sparkContext.parallelize([['p1', 'a'], ['p2', 'b'], ['p3', 'c']]).toDF(["other", "b_id"])
```

```{python eval=FALSE}
b.show()
```

```{python eval=FALSE}
# output 
+-----+----+
|other|b_id|
+-----+----+
|   p1|   a|
|   p2|   b|
|   p3|   c|
+-----+----+
```

```{python eval=FALSE}
c = a.join(b, a.a_id == b.b_id,'outer')
```

```{python eval=FALSE}
c.show()
```

```{python eval=FALSE}
# output
+----+-----+-----+----+
|a_id|extra|other|b_id|
+----+-----+-----+----+
|   c|  haw|   p3|   c|
|   b|  hem|   p2|   b|
|   a|  foo|   p1|   a|
+----+-----+-----+----+
```

