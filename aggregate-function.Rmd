---
title: "Aggregate Function"
author: "Wenqiang Feng & Ming Chen"
date: "2/7/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## `pyspark.RDD.aggregate`

```{python eval=FALSE}
aggregate(zeroValue, seqOp, combOp)
```

* `zeroValue` is a variable with user initialed values
* `seqOp` is a function which takes two variables: `zeroValue` and an RDD record (element). The purpose of function `seqOp` is to take the initial values and values from an RDD record and then do some calculation. The calculate results will be **added** to the previous `zeroValue` variable. This process will go through all the RDD elements in a partition.
* `combOp` is a function which takes two *dynamic* variables: `final-zeroValue-in-one-partition` and `final-zeroValue-in-another-partition`. Here I call it dynamic because this process go through all the partitions.

### Example of `pyspark.RDD.aggregate` usage

* **Data**: my data has 10 elements (rows) and 2 columns. Let's say, I want to get 3 values.
    + value 1: the sum of column 1 - column 2
    + value 2: the sum of column 1 + column 2
    + value 3: the sum of column 1 * column 2
    
```{python eval=FALSE}
myData = sc.parallelize([(1,2), (3,4), (5,6), (7,8), (9,10)])
myData.collect()
```

* **zeroValue**: my `zeroValue` could be a list or tuple which has three elements to store the three values. Let's set it as `(0, 0, 0)`

* **seqOp** and **combOp**

```{python eval=FALSE}
seqOp = (lambda z, r: (r[0] - r[1], r[0] + r[1], r[0] * r[1]))
combOp = (lambda px, py: (px[0] + py[0], px[1] + py[1], px[2] + py[2]))
```

* **Run `aggregate`**

```{python eval=FALSE}
myData.aggregate((0, 0, 0), seqOp, combOp)
```

* **Output**

```{python eval=FALSE}
(-5, 55, 190)
```


## `pyspark.RDD.aggregateByKey`

* To use this function, your data should be a **paired RDD**. That means, each element in your RDD should have a key and value, for example, a dictionary or a tuple with two elements.

```{python eval=FALSE}
byKeyData = sc.parallelize([('a', (1,2)),
                            ('a', (2,2)),
                            ('b', (3,4)), 
                            ('c', (5,6)),
                            ('c', (15,6)),
                            ('d', (7,8)), 
                            ('e', (9,10))])

byKeyData.collect()
```

* See the data below. Each element is a tuple. The first element can be a key, the second element is the corresponding value which is a tuple with two elements.

```{python eval=FALSE}
[('a', (1, 2)),
 ('a', (2, 2)),
 ('b', (3, 4)),
 ('c', (5, 6)),
 ('c', (15, 6)),
 ('d', (7, 8)),
 ('e', (9, 10))]
```



* `zeroValue` is a variable with user initialed values
* `seqOp` is a function which takes two variables: `zeroValue` and a **variable** which **refers to the value in an RDD record (element)**. You don't need to care about the **keys** in the element. 
* `combOp` is a function which takes two *dynamic* variables: `final-zeroValue-in-one-partition` and `final-zeroValue-in-another-partition`. Here I call it dynamic because this process go through all the partitions.


* **Here I would like to calculate three values:**
    + value 1: aggregate the first element in the value by key
    + value 2: aggregate the second element in the value by key
    + value 5: multiply the second element in the value by 5 and then aggregate the result by key
    
* **zeroValue**: (0,0,0).

* **seqOp** and **combOp**

```{python eval=FALSE}
seqOp = (lambda z,r: (r[0], r[1], r[1]*5))
combOp = (lambda px, py: (px[0] + py[0], px[1] + py[1], px[2] + py[2]))
```
    
* **Run `aggregateByKey`**

```{python eval=FALSE}
byKeyData.aggregateByKey((0,0,0), seqOp, combOp).collect()
```

* **Output**

```{python eval=FALSE}
[('a', (3, 4, 20)),
 ('c', (20, 12, 60)),
 ('b', (3, 4, 20)),
 ('e', (9, 10, 50)),
 ('d', (7, 8, 40))]
```

